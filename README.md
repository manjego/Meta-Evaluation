# Meta-Evaluation
Meta-Evaluation of Evaluation in Text Summarization

I have tried to perform the meta-evaluation of automated evaluation metrics used in text summarization.
Whole task has been done in two parts PART 1 Analysis and PART 2 Analysis. 

PART 1 Analysis is my work which consists of abstractive and extractive text summarization systems for generating the summaries of 100 news articles consisting of protest events in it. Then I have evaluated the generated summaries using the automated evaluation metrics ROUGE and BERTScore and tried comparing them with manual scores produced by the use of Pyramid Method.

PART 2 Analysis is the analysis of work Re-evaluating evaluation of text summarization by Bhandari et al.,2020.

In conclusion, I have found out that ROUGE and BERTscore are unable to perform the factual consistency check. And task-specific metrics are needed to be implmented.
